{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_color = (10,10,205)\n",
    "#(205,197,0) \n",
    "# 38,38,205 #black red\n",
    "# 0, 134, 139 #blue green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFaceBox(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    #print(detections.shape[2])\n",
    "    bboxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            cv.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), frame_color, int(round(frameHeight/150)), 1)\n",
    "    return frameOpencvDnn, bboxes\n",
    "\n",
    "\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "faceNet = cv.dnn.readNet(faceModel, faceProto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "genderList = ['Male', 'Female']\n",
    "genderNet = cv.dnn.readNet(genderModel, genderProto)\n",
    "faceNet = cv.dnn.readNet(faceModel, faceProto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True #\n",
    "STATE_DICT_PATH = \"./modelPath/model.pt\"\n",
    "GRAYSCALE = False #\n",
    "#dataset=afad,start=15,finish=41\n",
    "NUM_CLASSES = 26\n",
    "ADD_CLASS = 15\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "#\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "#\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.num_classes = num_classes\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1, padding=2)\n",
    "        self.fc = nn.Linear(2048 * block.expansion, (self.num_classes-1)*2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        logits = logits.view(-1, (self.num_classes-1), 2)\n",
    "        probas = F.softmax(logits, dim=2)[:, :, 1]\n",
    "        return logits, probas\n",
    "\n",
    "#\n",
    "def resnet34(num_classes, grayscale):\n",
    "    \"\"\"Constructs a ResNet-34 model.\"\"\"\n",
    "    model = ResNet(block=BasicBlock, \n",
    "                   layers=[3, 4, 6, 3],\n",
    "                   num_classes=num_classes,\n",
    "                   grayscale=grayscale)\n",
    "    return model\n",
    "\n",
    "#input<numpy.ndarray>/output age number\n",
    "def AgePredict(image_np):\n",
    "    image_PIL = Image.fromarray(image_np)\n",
    "    custom_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                       transforms.CenterCrop((120, 120)),\n",
    "                                       transforms.ToTensor()])\n",
    "    image = custom_transform(image_PIL)\n",
    "    DEVICE = torch.device('cpu')\n",
    "    image = image.to(DEVICE)\n",
    "    \n",
    "    model = resnet34(NUM_CLASSES, GRAYSCALE)\n",
    "    model.load_state_dict(torch.load(STATE_DICT_PATH, map_location=DEVICE))\n",
    "    model.eval()\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        logits, probas = model(image)\n",
    "        predict_levels = probas > 0.5\n",
    "        predicted_label = torch.sum(predict_levels, dim=1)\n",
    "        #print('Predicted age:', predicted_label.item() + ADD_CLASS)\n",
    "        agePredict = predicted_label.item() + ADD_CLASS\n",
    "        return agePredict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 2.287\n",
      "time : 1.264\n",
      "time : 0.970\n",
      "time : 1.926\n",
      "time : 0.883\n",
      "time : 1.650\n",
      "time : 2.283\n",
      "time : 1.479\n",
      "time : 0.828\n",
      "time : 1.508\n",
      "time : 2.100\n",
      "time : 2.116\n",
      "time : 1.425\n",
      "time : 1.461\n",
      "time : 0.787\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "time : 0.928\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "No face Detected, Checking next frame\n",
      "time : 0.965\n",
      "time : 0.769\n",
      "time : 0.755\n",
      "time : 0.745\n",
      "time : 0.773\n",
      "time : 0.753\n",
      "time : 0.776\n",
      "time : 0.735\n",
      "time : 0.733\n",
      "time : 0.793\n",
      "time : 0.762\n",
      "time : 0.813\n"
     ]
    }
   ],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "padding = 20\n",
    "while cv.waitKey(1) < 0:\n",
    "    t = time.time()\n",
    "    hasFrame, frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        cv.waitKey()\n",
    "        break\n",
    "\n",
    "    frameFace, bboxes = getFaceBox(faceNet, frame)\n",
    "    if not bboxes:\n",
    "        print(\"No face Detected, Checking next frame\")\n",
    "        continue\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        # print(bbox)\n",
    "        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "        \n",
    "        ##A\n",
    "        age=AgePredict(face)\n",
    "        \n",
    "        ##G\n",
    "        blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds = genderNet.forward()\n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "\n",
    "        label = \"{},{}\".format(gender, age)\n",
    "        cv.rectangle(frameFace, (bbox[0]-2, bbox[3]), (bbox[2]+2, bbox[3]+10), frame_color, -1)\n",
    "        cv.putText(frameFace, label, (bbox[0], bbox[3]+10), cv.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1, cv.LINE_AA)\n",
    "        cv.imshow(\"Age Gender Demo\", frameFace)\n",
    "\n",
    "    print(\"time : {:.3f}\".format(time.time() - t))\n",
    "    if cv.waitKey(20) & 0xFF == 27:\n",
    "        break \n",
    "    \n",
    "cap.release() \n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
